{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from sys import stdout\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "from Queue import Queue\n",
    "from threading import Thread\n",
    "from threading import current_thread\n",
    "\n",
    "MIN_EXPANSION_SIZE = 10\n",
    "MAX_DELETED_NEURONS = 3\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "##function to visualise graph\n",
    "def visualise_graph(G, colours, layer):\n",
    "        \n",
    "    ## create new figure for graph plot\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # graph layout\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    #attributes in this graph\n",
    "    attributes = np.unique([v for k, v in nx.get_node_attributes(G, \"assigned_community_layer_{}\".format(layer)).items()])\n",
    "\n",
    "    # draw nodes -- colouring by cluster\n",
    "    for i in range(min(len(colours), len(attributes))):\n",
    "       \n",
    "        node_list = [n for n in G.nodes() if G.node[n][\"assigned_community_layer_{}\".format(layer)] == attributes[i]]\n",
    "        colour = [colours[i] for n in range(len(node_list))]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=node_list, node_color=colour)\n",
    "        \n",
    "    #draw edges\n",
    "    nx.draw_networkx_edges(G, pos)\n",
    "\n",
    "    # draw labels\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "    \n",
    "    #title of plot\n",
    "    plt.title('Nodes coloured by cluster, layer: {}'.format(layer))\n",
    "\n",
    "    #show plot\n",
    "    plt.show()\n",
    "\n",
    "## visualise graph based on network clusters\n",
    "def visualise_network(network, colours, layer):\n",
    "    \n",
    "    #num neurons in lattice\n",
    "    num_neurons = len(network)\n",
    "\n",
    "    ##create new figure for lattice plot\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # graph layout\n",
    "    pos = nx.spring_layout(network)\n",
    "\n",
    "    # draw nodes -- colouring by cluster\n",
    "    for i in range(len(colours)):\n",
    "        nx.draw_networkx_nodes(network, pos, nodelist = [network.nodes()[i]], node_color = colours[i])\n",
    "\n",
    "    #draw edges\n",
    "    nx.draw_networkx_edges(network, pos)\n",
    "\n",
    "    # draw labels\n",
    "    nx.draw_networkx_labels(network, pos)\n",
    "    \n",
    "    #label axes\n",
    "    plt.title('Neurons in lattice, layer: '+str(layer))\n",
    "    \n",
    "    #show lattice plot\n",
    "    plt.show()\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "#function to generate real valued som for graph input\n",
    "#three initial nodes\n",
    "def initialise_network(ID, X, starting_nodes=3):\n",
    "    \n",
    "    #network will be a one dimensional list\n",
    "    network = nx.Graph(ID = ID)\n",
    "    \n",
    "    #initialise a network with just one neuron\n",
    "    network.add_nodes_from(range(1, starting_nodes + 1))\n",
    "    \n",
    "    #id of nodes\n",
    "    for n in network.nodes():\n",
    "        network.node[n][\"ID\"] = \"{}-{}\".format(ID, str(n).zfill(2))\n",
    "        \n",
    "    #connect nodes     \n",
    "    for i in range(1, starting_nodes + 1):\n",
    "        for j in range(i + 1, starting_nodes + 1):\n",
    "            network.add_edge(i, j)\n",
    "    \n",
    "    #assign a random vector in X to be the weight\n",
    "    V = X[np.random.randint(len(X), size=starting_nodes)]\n",
    "    \n",
    "    ##signal frequency\n",
    "    signal_frequencies = np.zeros(starting_nodes)\n",
    "\n",
    "    return network, V\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "def precompute_sigmas(sigma, num_epochs):\n",
    "    \n",
    "    return np.array([sigma * np.exp(-2 * sigma * e / num_epochs)\n",
    "                     for e in range(num_epochs)])\n",
    "\n",
    "##########################################################################################################################\n",
    "##TODO\n",
    "# function to train SOM on given graph\n",
    "def train_network(X, network, V, num_epochs, eta_0, precomputed_sigmas):\n",
    "    \n",
    "    #initial learning rate\n",
    "    eta = eta_0\n",
    "    \n",
    "    #list if all patterns to visit\n",
    "    training_patterns = range(len(X))\n",
    "    \n",
    "    #shortest path matrix\n",
    "    shortest_path = np.array(nx.floyd_warshall_numpy(network))\n",
    "    \n",
    "#     net_change = np.zeros((V.shape))\n",
    "\n",
    "    signal_frequencies = np.zeros(len(network))\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        #shuffle nodes\n",
    "        np.random.shuffle(training_patterns)\n",
    "        \n",
    "        sigma = precomputed_sigmas[e]\n",
    "        \n",
    "        # iterate through N nodes of graph\n",
    "        for i in training_patterns:\n",
    "            \n",
    "            #data point to consider\n",
    "            x = X[i]\n",
    "            \n",
    "            #determine winning neuron\n",
    "            closest_neuron = winning_neuron(x, V)\n",
    "            \n",
    "            #increment signal frequency of winning neuron\n",
    "            signal_frequencies[closest_neuron] += 1\n",
    "            \n",
    "            # update weights\n",
    "            deltaV = update_weights(x, V, closest_neuron, shortest_path[closest_neuron], eta, sigma)\n",
    "            \n",
    "            #weight update (vectorised)\n",
    "            V += deltaV\n",
    "            \n",
    "            #decay signal frequencies\n",
    "            signal_frequencies -= 0.001 * signal_frequencies\n",
    "\n",
    "#             net_change += deltaV\n",
    "            \n",
    "    print \"TRAINING COMPLETED\"\n",
    "    print signal_frequencies\n",
    "#     print net_change\n",
    "#     print np.linalg.norm(net_change, axis=1)\n",
    "            \n",
    "    return V, signal_frequencies\n",
    "        \n",
    "##########################################################################################################################\n",
    "\n",
    "# winning neuron\n",
    "def winning_neuron(x, V):\n",
    "    \n",
    "    distances = np.linalg.norm(x - V, axis=1)\n",
    "    \n",
    "    return distances.argmin()\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "# function to update weights\n",
    "def update_weights(x, V, winning_neuron, shortest_path_length, eta, sigma):\n",
    "    \n",
    "    #weight update (vectorised)\n",
    "    return np.dot(np.diag(eta * np.exp(- shortest_path_length ** 2 / (2 * sigma ** 2))), \n",
    "                      (x - V))\n",
    "\n",
    "########################################################################################################################   \n",
    "\n",
    "def compute_relative_signal_frequencies(signal_frequencies):\n",
    "    return signal_frequencies / sum(signal_frequencies)\n",
    "\n",
    "def compute_probabilities(network, V, relative_signal_frequencies):\n",
    "    \n",
    "    #dimension of data\n",
    "    num_neurons, d = V.shape\n",
    "    \n",
    "    #distances between all weight vectors of neurons\n",
    "    distances = compute_euclidean_distances(network, V)\n",
    "    \n",
    "    #neighours of all neurons in network\n",
    "    neighbours = [network.neighbors(n) for n in network.nodes()]\n",
    "    \n",
    "    print \"NEIGHBOURS\"\n",
    "    print neighbours\n",
    "    \n",
    "    #mean distance for each neruon\n",
    "    volumes = np.array([np.mean(np.array([distances[network.nodes()[i]][neighbour]\n",
    "                                       for neighbour in neighbours[i]])) for i in range(num_neurons)])\n",
    "    \n",
    "    print \"VOLUMES\"\n",
    "    print volumes\n",
    "\n",
    "    volumes = volumes ** d    \n",
    "    print \"VOLUMES\"\n",
    "    print volumes\n",
    "    \n",
    "    probabilities = relative_signal_frequencies / (volumes / sum(volumes))\n",
    "    \n",
    "    return probabilities / sum(probabilities)\n",
    "    \n",
    "    \n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "# assign nodes into clusters\n",
    "def assign_nodes(names, X, network, V, signal_frequencies, probabilities):\n",
    "    \n",
    "    #pairwise distances\n",
    "    arg_min_distances, min_distances = pairwise_distances_argmin_min(X, V)\n",
    "\n",
    "    #nodes corresponding to minimum index (of length len(X))\n",
    "    minimum_nodes = np.array([network.nodes()[n] for n in arg_min_distances])\n",
    "\n",
    "    #list of neurons with no assignments\n",
    "    empty_neurons = np.array([n for n in network.nodes() if n not in minimum_nodes])\n",
    "    \n",
    "    if empty_neurons.size > 0:\n",
    "    \n",
    "        ################################################DELETION####################################################\n",
    "\n",
    "        #neighbours of deleted neurons\n",
    "        neighbour_lists = np.array([network.neighbors(n) for n in empty_neurons])\n",
    "        \n",
    "        print \"DELETING NODES: {}\".format(empty_neurons)\n",
    "        \n",
    "        #remove the nodes\n",
    "        network.remove_nodes_from(empty_neurons)\n",
    "        \n",
    "        ##remove from V\n",
    "        V = np.array([V[i] for i in range(len(V)) if i in arg_min_distances])\n",
    "        \n",
    "        #compute distances between all neurons in input space\n",
    "        computed_neuron_distances = compute_euclidean_distances(network, V)\n",
    "        \n",
    "        ##connect separated components\n",
    "        for neighbour_list in neighbour_lists:\n",
    "            connect_components(network, neighbour_list, computed_neuron_distances)\n",
    "\n",
    "        ############################################################################################################\n",
    "\n",
    "    #array of errors\n",
    "    errors = np.array([np.mean(min_distances[minimum_nodes == n]) for n in network.nodes()])\n",
    "    \n",
    "    #compute MQE\n",
    "    MQE = np.mean(errors)\n",
    "    \n",
    "    print \"MQE={}, size of map={}\".format(MQE, len(network))\n",
    "    \n",
    "    ##array of assignments\n",
    "    assignments_array = np.array([np.array([names[i] for i in np.where(minimum_nodes == n)[0]]) for n in network.nodes()])\n",
    "    \n",
    "    #zip zith nodes\n",
    "    errors = {n: e for n, e in zip(network.nodes(), errors)}\n",
    "    assignments = {n: a for n, a in zip(network.nodes(), assignments_array)}\n",
    "    \n",
    "    print \"ERRORS\"\n",
    "    print errors\n",
    "    print \"PROBABILITIES\"\n",
    "    print probabilities\n",
    "    print \"number of nodes assigned to neurons\"\n",
    "    print {n: len(a) for n, a in zip(network.nodes(), assignments_array)}\n",
    "    \n",
    "    nx.set_node_attributes(network, \"e\", errors)\n",
    "    nx.set_node_attributes(network, \"ls\", assignments)\n",
    "    \n",
    "    return MQE, empty_neurons.size, V\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "def compute_euclidean_distances(network, V):\n",
    "    \n",
    "    distances = euclidean_distances(V)\n",
    "    \n",
    "    return {network.nodes()[i] : {network.nodes()[j] : distances[i, j] for j in range(len(distances[i]))}\n",
    "           for i in range(len(distances))}\n",
    "    \n",
    "######################################################################################################################### \n",
    "\n",
    "\n",
    "def connect_components(network, neighbour_list, computed_neuron_distances):\n",
    "    \n",
    "    sub_network = network.subgraph(neighbour_list)\n",
    "    \n",
    "    connected_components = [sub_network.subgraph(c) for c in nx.connected_components(sub_network)]\n",
    "    number_of_connected_components = len(connected_components)\n",
    "    \n",
    "    for i in range(number_of_connected_components):\n",
    "        \n",
    "        connected_component_1 = connected_components[i].nodes()\n",
    "        \n",
    "        for j in range(i + 1, number_of_connected_components):\n",
    "            \n",
    "            connected_component_2 = connected_components[j].nodes()\n",
    "            \n",
    "            distances = np.array([[computed_neuron_distances[n1][n2] for n2 in connected_component_2]\n",
    "                                 for n1 in connected_component_1])\n",
    "            \n",
    "            min_n1, min_n2 = np.unravel_index(distances.argmin(), distances.shape)\n",
    "            \n",
    "            network.add_edge(connected_component_1[min_n1], \n",
    "                            connected_component_2[min_n2])\n",
    "\n",
    "##########################################################################################################################\n",
    "            \n",
    "##function to identify neuron with greatest error\n",
    "def identify_error_unit(network, probabilities):\n",
    "    \n",
    "    print \"ERROR UNIT\"\n",
    "    print network.nodes()[probabilities.argmax()]\n",
    "    \n",
    "    return network.nodes()[probabilities.argmax()]\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "def expand_network(ID, named_X, network, V, signal_frequencies, error_unit):\n",
    "    \n",
    "    #v goes to random vector in range of error unit\n",
    "    ls = network.node[error_unit][\"ls\"]    \n",
    "    r = np.random.randint(len(ls))\n",
    "    v = named_X[ls[r]]\n",
    "    \n",
    "    #zip nodes and distances\n",
    "    distances = zip(network.nodes(), np.linalg.norm(V - v, axis=1))\n",
    "        \n",
    "    #identify neighbour pointing closet\n",
    "    error_unit_neighbours = network.neighbors(error_unit)\n",
    "    \n",
    "    \n",
    "    #id of new node\n",
    "    new_node = max(network) + 1\n",
    "    \n",
    "    #add new node to map\n",
    "    network.add_node(new_node)\n",
    "    \n",
    "    ##id\n",
    "    network.node[new_node][\"ID\"] = \"{}-{}\".format(ID, str(new_node).zfill(2))\n",
    "    \n",
    "    #add edges to map\n",
    "    \n",
    "    #connect error unit and new node\n",
    "    network.add_edge(error_unit, new_node)\n",
    "    \n",
    "    if len(error_unit_neighbours) > 0:\n",
    "        \n",
    "        ##find closest neighbour\n",
    "        distances = {n: v for n, v in distances if n in error_unit_neighbours}\n",
    "        closest_neighbour = min(distances, key=distances.get)\n",
    "        \n",
    "        #connect to error unit and closest neighbour\n",
    "        network.add_edge(closest_neighbour, new_node)\n",
    "        \n",
    "    #add v to V\n",
    "    V = np.vstack([V, v])  \n",
    "    \n",
    "    #signal frequency of new neuron\n",
    "    signal_frequencies = np.append(signal_frequencies, 0)\n",
    "    \n",
    "    return V, signal_frequencies\n",
    "        \n",
    "##########################################################################################################################\n",
    "##########################################################################################################################\n",
    "\n",
    "##GHSOM algorithm\n",
    "def ghsom(ID, named_X, num_iter, eta, sigma, e_0, e_sg, e_en, q):\n",
    "    \n",
    "    print \"MQE_0={}, growth target={}\".format(e_0, e_0 * e_sg)\n",
    "    \n",
    "    #separate names and matrix of node embedding\n",
    "    names, X = zip(*named_X.items())\n",
    "    names = np.array(names)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    #create som for this neuron\n",
    "    network, V, = initialise_network(ID, X)\n",
    "    \n",
    "    #precompute sigmas\n",
    "    precomputed_sigmas = precompute_sigmas(sigma, num_iter)\n",
    "    \n",
    "    #train for lamda epochs\n",
    "    V, signal_frequencies = train_network(X, network, V, num_iter, eta, precomputed_sigmas)\n",
    "    \n",
    "    #normalise signal frequencies\n",
    "    signal_frequencies = compute_relative_signal_frequencies(signal_frequencies)\n",
    "    \n",
    "    print \"RELATIVE SIGNAL FREQUENCIES\"\n",
    "    print signal_frequencies\n",
    "\n",
    "    #compute probabilities\n",
    "    probabilities = compute_probabilities(network, V, signal_frequencies)\n",
    "    \n",
    "    print \"PROBABILITIES\"\n",
    "    print probabilities\n",
    "    \n",
    "    #classify nodes and compute error\n",
    "    MQE, num_deleted_neurons, V = assign_nodes(names, X, network, V, signal_frequencies, probabilities)\n",
    "    \n",
    "    ##som growth phase\n",
    "    #repeat until error is low enough\n",
    "    while MQE > e_sg * e_0 and num_deleted_neurons < MAX_DELETED_NEURONS:\n",
    "        \n",
    "        #find neuron with greatest error\n",
    "        error_unit = identify_error_unit(network, probabilities)\n",
    "        \n",
    "        #expand network\n",
    "        V, signal_frequencies = expand_network(ID, named_X, network, V, signal_frequencies, error_unit)\n",
    "        \n",
    "        #train for lam epochs\n",
    "        V, signal_frequencies = train_network(X, network, V, snum_iter, eta, precomputed_sigmas)\n",
    "        \n",
    "        #normalise signal frequencies\n",
    "        signal_frequencies = compute_relative_signal_frequencies(signal_frequencies)\n",
    "\n",
    "        print \"RELATIVE SIGNAL FREQUENCIES\"\n",
    "        print signal_frequencies\n",
    "\n",
    "        #compute probabilities\n",
    "        probabilities = compute_probabilities(network, V, signal_frequencies)\n",
    "\n",
    "        print \"PROBABILITIES\"\n",
    "        print probabilities\n",
    "\n",
    "        #calculate mean network error\n",
    "        MQE, deleted_neurons, V = assign_nodes(names, X, network, V, signal_frequencies, probabilities)\n",
    "        num_deleted_neurons += deleted_neurons\n",
    "        \n",
    "    print \"growth terminated, MQE: {}, target: {}, number of deleted neurons: {}\".format(MQE, \n",
    "                                            e_0 * e_sg, num_deleted_neurons)\n",
    "        \n",
    "    ##neuron expansion phase\n",
    "    #iterate thorugh all neruons and find neurons with error great enough to expand\n",
    "    for _, d in network.nodes(data=True):\n",
    "        \n",
    "        #unpack\n",
    "        node_id = d[\"ID\"]\n",
    "        ls = d[\"ls\"]\n",
    "        e = d[\"e\"]\n",
    "        \n",
    "        #check error\n",
    "        if (e > e_en * e_0 and len(ls) > MIN_EXPANSION_SIZE and num_deleted_neurons < MAX_DELETED_NEURONS):\n",
    "            \n",
    "#             id = \"{}-{}\".format(ID, node_id) \n",
    "                \n",
    "            sub_X = {k: named_X[k] for k in ls}\n",
    "            \n",
    "            print \"submitted job: ID={}, e={}, number of nodes={}\".format(node_id, e, len(ls))\n",
    "            \n",
    "            #add these parameters to the queue\n",
    "            q.put((node_id, sub_X, num_iter, eta, sigma, e, e_sg, e_en))\n",
    "    \n",
    "    #return network\n",
    "    return network, MQE\n",
    "\n",
    "##########################################################################################################################\n",
    "##########################################################################################################################\n",
    "\n",
    "def label_nodes(G, networks):\n",
    "    \n",
    "    for _, network, _ in networks: \n",
    "        \n",
    "        for _, d in network.nodes(data=True):\n",
    "            \n",
    "            community = d[\"ID\"]\n",
    "            layer = community.count(\"-\")\n",
    "            assignment_string = \"assigned_community_layer_{}\".format(layer)\n",
    "            \n",
    "            for node in d[\"ls\"]:\n",
    "                \n",
    "                G.node[node][assignment_string] = community\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "def NMI_one_layer(G, label, layer):\n",
    "    \n",
    "    #actual community for this layer\n",
    "    actual_community_labels = np.array([v for k, v in nx.get_node_attributes(G, label).items()])\n",
    "    \n",
    "    #predicted communitiy for this layer\n",
    "    predicted_community_labels = np.array([v for k, v in nx.get_node_attributes(G, \n",
    "         \"assigned_community_layer_{}\".format(layer)).items()])\n",
    "\n",
    "    print actual_community_labels\n",
    "    print predicted_community_labels\n",
    "    \n",
    "    return met.normalized_mutual_info_score(actual_community_labels, predicted_community_labels)\n",
    "\n",
    "def NMI_all_layers(G, labels):\n",
    "    \n",
    "    return np.array([NMI_one_layer(G, labels[i], i + 1) for i in range(len(labels))])\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "## get embedding\n",
    "def get_embedding(G):\n",
    "\n",
    "    return np.array([v for k, v in nx.get_node_attributes(G, \"embedding\").items()])\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "def process_job(q, networks):\n",
    "    \n",
    "    #unpack first element of queue\n",
    "    #contains all the para,eters for GHSOM\n",
    "    ID, X, num_iter, eta, sigma, e_0, e_sg, e_en = q.get()\n",
    "\n",
    "    #run GHSOM and return a network and MQE\n",
    "    n, e = ghsom(ID, X, num_iter, eta, sigma, e_0, e_sg, e_en, q)\n",
    "\n",
    "    #append result to networks list\n",
    "    networks.append((ID, n, e))\n",
    "\n",
    "    #mark task as done\n",
    "    q.task_done()\n",
    "\n",
    "def worker(q, networks):\n",
    "    \n",
    "    #continually poll queue for jobs \n",
    "    while True:\n",
    "        process_job(q, networks)\n",
    "\n",
    "def main(params, filename, num_iter=10000, num_threads=1):\n",
    "    \n",
    "    #network\n",
    "    G = nx.read_gpickle(filename)\n",
    "    \n",
    "    #embedding matrix\n",
    "    X = get_embedding(G)\n",
    "    \n",
    "    #zip with names\n",
    "    named_X = {k: v for k, v in zip(G.nodes(), X)}\n",
    "    \n",
    "    ##list of returned networks\n",
    "    networks = []\n",
    "    \n",
    "    #initilise worker queue\n",
    "    q = Queue()\n",
    "    \n",
    "    ##initial MQE is variance of dataset\n",
    "    m = np.mean(X, axis=0)\n",
    "    MQE_0 = np.mean(np.linalg.norm(X - m, axis=1))\n",
    "    \n",
    "    #add initial layer of ghsom to queue\n",
    "    q.put((\"01\", named_X, num_iter, params[\"eta\"], params[\"sigma\"], MQE_0, params[\"e_sg\"], params[\"e_en\"]))\n",
    "    \n",
    "    if num_threads > 1:\n",
    "    \n",
    "        #initialise threads\n",
    "        for i in range(num_threads):\n",
    "\n",
    "            t = Thread(target=worker, args=(q, networks))\n",
    "            t.setDaemon(True)\n",
    "            t.start()\n",
    "\n",
    "        #finally wait until queue is empty and all tasks are done\n",
    "        q.join()\n",
    "        \n",
    "    else :\n",
    "        \n",
    "        #single thread\n",
    "        while not q.empty():\n",
    "            process_job(q, networks)\n",
    "    \n",
    "    print \"DONE\"\n",
    "    \n",
    "    return G, networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# params = {'eta': 0.0001,\n",
    "#          'sigma': 1,\n",
    "#           'e_sg': 0.7,\n",
    "#          'e_en': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MQE_0=3.08239724, growth target=2.157678068\n",
      "MQE=2.72729591788, size of map=3\n",
      "MQE=2.5967012766, size of map=4\n",
      "MQE=2.46397530064, size of map=5\n",
      "MQE=2.34206565231, size of map=6\n",
      "MQE=2.29204360073, size of map=7\n",
      "MQE=2.16170680158, size of map=8\n",
      "MQE=2.17000212218, size of map=9\n",
      "MQE=2.08837745274, size of map=10\n",
      "growth terminated, MQE: 2.08837745274, target: 2.157678068, number of deleted neurons: 0\n",
      "DONE\n",
      " "
     ]
    }
   ],
   "source": [
    "# %prun G, networks = main(params=params, filename=\"embedded_benchmark.gpickle\", num_threads=1, num_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# label_nodes(G, networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NMI_all_layers(G, labels=[\"firstlevelcommunity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _, network, _ = networks[0]\n",
    "# colours = np.random.rand(len(network), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualise_graph(G=G, colours=colours, layer=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
