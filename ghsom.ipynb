{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from sys import stdout\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import sklearn.metrics as met\n",
    "import sklearn.manifold as man\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MIN_EXPANSION_SIZE = 10\n",
    "\n",
    "#function to generate real valued som for graph input\n",
    "def initialise_network(X, num_neurons):\n",
    "    \n",
    "    #network will be a one dimensional list\n",
    "    network = nx.Graph()\n",
    "    \n",
    "    #number of data points\n",
    "    num_nodes = len(X) \n",
    "    \n",
    "    #dimension of data in X\n",
    "    d = len(X[0])\n",
    "    \n",
    "    #regular lattice\n",
    "    lattice_size = np.floor(np.sqrt(num_neurons))\n",
    "    \n",
    "    for i in range(num_neurons):\n",
    "        \n",
    "        ##position\n",
    "        network.add_node(i)\n",
    "        \n",
    "        ##weight    \n",
    "#         network.node[i]['v'] = 2 * (np.random.rand(d) - 0.5) * w\n",
    "        r = np.random.randint(num_nodes)\n",
    "        network.node[i]['v'] = X[r]\n",
    "\n",
    "        ##list of closest nodes\n",
    "        network.node[i]['ls'] = []\n",
    "\n",
    "        ##error of neuron\n",
    "        network.node[i]['e'] = 0\n",
    "        \n",
    "        ##som for neuron\n",
    "        network.node[i]['n'] = []\n",
    "        \n",
    "        #connections\n",
    "        if i % lattice_size > 0:\n",
    "            #horizontal connection\n",
    "            network.add_edge(i, i - 1)\n",
    "        \n",
    "        if i >= lattice_size:\n",
    "            #vertical connection\n",
    "            network.add_edge(i, i - lattice_size)\n",
    "            \n",
    "            if i % lattice_size < lattice_size - 1:\n",
    "                #diagonal connection\n",
    "                network.add_edge(i, i - lattice_size + 1)\n",
    "                \n",
    "    \n",
    "    #return network\n",
    "    return network\n",
    "\n",
    "\n",
    "# function to train SOM on given graph\n",
    "def train_network(X, network, num_epochs, eta_0, sigma_0, N, layer, MQE, target):\n",
    "    \n",
    "    #initial learning rate\n",
    "    eta = eta_0\n",
    "    \n",
    "    #initial neighbourhood size\n",
    "    sigma = sigma_0\n",
    "    \n",
    "    #list if all patterns to visit\n",
    "    training_patterns = [p for p in range(len(X))]\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        #shuffle nodes\n",
    "        np.random.shuffle(training_patterns)\n",
    "        \n",
    "        # iterate through N nodes of graph\n",
    "        for i in range(N):\n",
    "            \n",
    "            #data point to consider\n",
    "            x = X[training_patterns[i]]\n",
    "            \n",
    "            #determine winning neuron\n",
    "            win_neuron = winning_neuron(x, network)\n",
    "            \n",
    "            # update weights\n",
    "            update_weights(x, network, win_neuron, eta, sigma)\n",
    "            \n",
    "        # drop neighbourhood\n",
    "        sigma = sigma_0 * np.exp(-2 * sigma_0 * e / num_epochs);\n",
    "        \n",
    "        stdout.write(\"\\rLayer: {}, training epoch: {}/{}, size of map: {}, network size: {}, MQE: {}, target: {}\".format(layer,\n",
    "                        e, num_epochs, len(network), len(X), MQE, target) + \" \" * 20)\n",
    "\n",
    "# winning neuron\n",
    "def winning_neuron(x, network):\n",
    "\n",
    "    # minimum distance so far\n",
    "    min_dist = np.inf\n",
    "    winning_neuron = []\n",
    "    \n",
    "    # iterate through network\n",
    "    for i in network.nodes():\n",
    "            \n",
    "        #unpack network\n",
    "        v = network.node[i]['v']\n",
    "\n",
    "        #distance between input vector and neuron weight\n",
    "        distance = np.linalg.norm(x - v)\n",
    "\n",
    "        # if we have a new closest neuron\n",
    "        if distance < min_dist:\n",
    "            min_dist = distance\n",
    "            winning_neuron = i\n",
    "    \n",
    "    #return\n",
    "    return winning_neuron\n",
    "\n",
    "# function to update weights\n",
    "def update_weights(x, network, win_neuron, eta, sigma):\n",
    "    \n",
    "    # iterate through all neurons in network\n",
    "    for i in network.nodes():\n",
    "        \n",
    "        #unpack\n",
    "        v = network.node[i]['v']\n",
    "\n",
    "        #new v -- move along shortest path by move distance\n",
    "        v += eta * neighbourhood(network, i, win_neuron, sigma) * (x - v)\n",
    "\n",
    "        #save to network\n",
    "        network.node[i]['v'] = v\n",
    "\n",
    "# neighbourhood function\n",
    "def neighbourhood(network, r, win_neuron, sigma):\n",
    "    \n",
    "    return np.exp(-(nx.shortest_path_length(network, r, win_neuron)) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "# assign nodes into clusters\n",
    "def assign_nodes(G, X, network, layer):\n",
    "    \n",
    "    #number of neurons in network\n",
    "    num_neurons = nx.number_of_nodes(network)\n",
    "    \n",
    "    #number of nodes\n",
    "    num_nodes = nx.number_of_nodes(G)\n",
    "    \n",
    "    # clear existing closest node list\n",
    "    for i in network.nodes():\n",
    "        \n",
    "        network.node[i]['ls'] = []\n",
    "        network.node[i]['e'] = 0\n",
    "    \n",
    "    # assign colour to each node\n",
    "    for n in range(num_nodes):\n",
    "        \n",
    "        #data point to assign\n",
    "        x = X[n]\n",
    "    \n",
    "        #intialise distance to be infinity\n",
    "        min_distance = np.inf\n",
    "        \n",
    "        #closest reference vector to this ndoe\n",
    "        closest_ref = []\n",
    "\n",
    "        # find which neuron's referece vector this node is closest to\n",
    "        for i in network.nodes():\n",
    "                \n",
    "            #unpack network\n",
    "            v = network.node[i]['v']\n",
    "\n",
    "            # calculate distance to that reference vector\n",
    "            d = np.linalg.norm(x - v)\n",
    "\n",
    "            if d < min_distance:\n",
    "                min_distance = d\n",
    "                closest_ref = i\n",
    "        \n",
    "        #unable to find closest ref\n",
    "        if closest_ref == []:\n",
    "            continue\n",
    "        \n",
    "        #add node to closest nodes list\n",
    "        network.node[closest_ref]['ls'].append(G.nodes()[n])\n",
    "        \n",
    "        #increase e by distance\n",
    "        network.node[closest_ref]['e'] += min_distance\n",
    "\n",
    "##function to return lattice grid of errors\n",
    "def update_errors(network, num_deleted_neurons):\n",
    "    \n",
    "    #mean network error\n",
    "    mqe = 0;\n",
    "    \n",
    "    #neurons with assigned nodes\n",
    "    num_neurons = 0\n",
    "    \n",
    "    #iterate over all neurons and average distance\n",
    "    for i in network.nodes():\n",
    "            \n",
    "        #unpack network\n",
    "        e = network.node[i]['e']\n",
    "        ls = network.node[i]['ls']\n",
    "        \n",
    "        if len(ls) == 0:\n",
    "            delete_node(network, i)\n",
    "            num_deleted_neurons += 1\n",
    "            continue\n",
    "            \n",
    "        num_neurons += 1\n",
    "\n",
    "        #divide by len(ls) for mean\n",
    "        e /= len(ls)\n",
    "\n",
    "        #sum total errors\n",
    "        mqe += e\n",
    "        \n",
    "        #save error to network\n",
    "        network.node[i]['e'] = e        \n",
    "    \n",
    "    #mean\n",
    "    mqe /= num_neurons\n",
    "    \n",
    "    return mqe, num_deleted_neurons\n",
    "\n",
    "def connect_closest_neurons(network, s1, s2):\n",
    "    \n",
    "    min_dist = np.inf\n",
    "    \n",
    "    c1 = []\n",
    "    c2 = []\n",
    "    \n",
    "    for i in s1:\n",
    "        \n",
    "        v1 = network.node[i]['v']\n",
    "        \n",
    "        for j in s2:\n",
    "            \n",
    "            v2 = network.node[j]['v']\n",
    "            \n",
    "            d = np.linalg.norm(v1 - v2)\n",
    "            \n",
    "            if d < min_dist:\n",
    "                \n",
    "                min_dist = d\n",
    "                c1 = i\n",
    "                c2 = j\n",
    "            \n",
    "    ##connect\n",
    "    network.add_edge(c1, c2)\n",
    "    \n",
    "def intersection(a, b):\n",
    "     return list(set(a) & set(b))\n",
    "\n",
    "def delete_node(network, n):\n",
    "    \n",
    "    neighbours = network.neighbors(n)\n",
    "    \n",
    "    network.remove_node(n)\n",
    "    \n",
    "    if not neighbours:\n",
    "        return\n",
    "    \n",
    "    components = [c for c in nx.connected_components(network)]\n",
    "    \n",
    "    for i in range(len(components)):\n",
    "        \n",
    "        conn_neigh_1 = intersection(components[i], neighbours)\n",
    "        \n",
    "        for j in range(i + 1, len(components)):\n",
    "            \n",
    "            conn_neigh_2 = intersection(components[j], neighbours)\n",
    "            \n",
    "            ##make connection between closest neurons in input space\n",
    "            connect_closest_neurons(network, conn_neigh_1, conn_neigh_2)\n",
    "            \n",
    "    \n",
    "            \n",
    "##function to identify neuron with greatest error\n",
    "def identify_error_unit(network):\n",
    "    \n",
    "    #initial value for maximum error found\n",
    "    max_e = 0\n",
    "    \n",
    "    #initial index to return\n",
    "    error_node = []\n",
    "    \n",
    "    for i in network.nodes():\n",
    "        \n",
    "        #unpack\n",
    "        e = network.node[i]['e']\n",
    "        \n",
    "        #check if this unit has greater error than maximum \n",
    "        if e > max_e:\n",
    "            \n",
    "            max_e = e\n",
    "            error_node = i\n",
    "            \n",
    "    #return id of unit with maximum error\n",
    "    return error_node\n",
    "\n",
    "def get_vector(node):\n",
    "    \n",
    "    d = 0\n",
    "    \n",
    "    while 'embedding'+str(d) in node:\n",
    "        d += 1\n",
    "    \n",
    "    v = np.zeros(d)\n",
    "    \n",
    "    for i in range(d):\n",
    "        v[i] = node['embedding{}'.format(i)]\n",
    "        \n",
    "    return v\n",
    "\n",
    "def expand_network(G, network, error_unit):\n",
    "    \n",
    "    #id of new node\n",
    "    id = max(network) + 1\n",
    "    \n",
    "    network.add_node(id)\n",
    "    \n",
    "    #v goes to random vector in range of error unit\n",
    "    ls = network.node[error_unit]['ls']\n",
    "    r = np.random.randint(len(ls))\n",
    "    node = G.node[ls[r]]\n",
    "    v = get_vector(node)\n",
    "    network.node[id]['v'] = v\n",
    "    \n",
    "    ##list of closest nodes\n",
    "    ls = []\n",
    "    network.node[id]['ls'] = ls\n",
    "\n",
    "    ##error of neuron\n",
    "    e = 0\n",
    "    network.node[id]['e'] = e\n",
    "\n",
    "    ##som for neuron\n",
    "    n = []\n",
    "    network.node[id]['n'] = n\n",
    "    \n",
    "    #connections to other neurons\n",
    "        \n",
    "    #identify neighbour pointing furthest away\n",
    "    error_unit_neighbours = network.neighbors(error_unit)\n",
    "    \n",
    "    if len(error_unit_neighbours) == 0:\n",
    "        \n",
    "        ##add edge\n",
    "        network.add_edge(error_unit, id)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        ##find closest neighbour\n",
    "        n = closest_neuron(network, error_unit, error_unit_neighbours)\n",
    "        \n",
    "        #connect to error unit and closest neighbour\n",
    "        network.add_edge(n, id)\n",
    "        network.add_edge(error_unit, id)\n",
    "\n",
    "\n",
    "##function to find neuron pointing furthest away in list\n",
    "def furthest_neuron(network, error_unit, ls):\n",
    "    \n",
    "    vi = network.node[error_unit]['v']\n",
    "    max_dist = -np.inf\n",
    "    \n",
    "    #neighbour id\n",
    "    furthest_node = []\n",
    "\n",
    "    #iterate through neighbours\n",
    "    for i in ls:\n",
    "\n",
    "        #unpack neighbour\n",
    "        v = network.node[i]['v']\n",
    "\n",
    "        #distance in input space\n",
    "        d = np.linalg.norm(v - vi)\n",
    "\n",
    "        #is d > max_dist?\n",
    "        if d > max_dist:\n",
    "\n",
    "            max_dist = d\n",
    "            furthest_node = i\n",
    "            \n",
    "    return furthest_node\n",
    "\n",
    "def closest_neuron(network, error_unit, ls):\n",
    "    \n",
    "    vi = network.node[error_unit]['v']\n",
    "    min_dist = np.inf\n",
    "    \n",
    "    #neighbour id\n",
    "    closest_node = []\n",
    "\n",
    "    #iterate through neighbours\n",
    "    for i in ls:\n",
    "\n",
    "        #unpack neighbour\n",
    "        v = network.node[i]['v']\n",
    "\n",
    "        #distance in input space\n",
    "        d = np.linalg.norm(v - vi)\n",
    "\n",
    "        #is d > max_dist?\n",
    "        if d < min_dist:\n",
    "\n",
    "            min_dist = d\n",
    "            closest_node = i\n",
    "            \n",
    "    return closest_node\n",
    "\n",
    "##GHSOM algorithm\n",
    "def ghsom(G, lam, eta, sigma, e_0, e_sg, e_en, init, layer):\n",
    "    \n",
    "    #embedding\n",
    "    X = get_embedding(G)\n",
    "    \n",
    "    #number of nodes in G\n",
    "    num_nodes = nx.number_of_nodes(G)\n",
    "    \n",
    "    ##number of training patterns to visit\n",
    "#     N = min(num_nodes, 100)\n",
    "    N = num_nodes\n",
    "    \n",
    "    #create som for this neuron\n",
    "    network = initialise_network(X, init)\n",
    "    \n",
    "    ##inital training phase\n",
    "    \n",
    "    #meal quantization error\n",
    "    MQE = np.inf\n",
    "    \n",
    "    #number of neurons deleted from the map\n",
    "    num_deleted_neurons = 0\n",
    "    \n",
    "    #train for lam epochs\n",
    "    train_network(X, network, lam, eta, sigma, N, layer, MQE, e_sg * e_0)\n",
    "\n",
    "    #classify nodes\n",
    "    assign_nodes(G, X, network, layer)\n",
    "\n",
    "    #calculate mean network error\n",
    "    MQE, num_deleted_neurons = update_errors(network, num_deleted_neurons)\n",
    "    \n",
    "    ##som growth phase\n",
    "    #repeat until error is low enough\n",
    "    while MQE > e_sg * e_0 and num_deleted_neurons < 3:\n",
    "    \n",
    "        #find neuron with greatest error\n",
    "        error_unit = identify_error_unit(network)\n",
    "        \n",
    "        #expand network\n",
    "#         expand_network(network, error_unit)\n",
    "        expand_network(G, network, error_unit)\n",
    "        \n",
    "        #train for l epochs\n",
    "        train_network(X, network, lam, eta, sigma, N, layer, MQE, e_sg * e_0)\n",
    "\n",
    "        #classify nodes\n",
    "        assign_nodes(G, X, network, layer)\n",
    "\n",
    "        #calculate mean network error\n",
    "        MQE, num_deleted_neurons = update_errors(network, num_deleted_neurons)\n",
    "    \n",
    "    #recalculate error after neuron expansion\n",
    "    MQE = 0\n",
    "    \n",
    "    ##neuron expansion phase\n",
    "    #iterate thorugh all neruons and find neurons with error great enough to expand\n",
    "    for i in range(len(network)):\n",
    "        \n",
    "        #unpack\n",
    "        ls = network.node[network.nodes()[i]]['ls']\n",
    "        e = network.node[network.nodes()[i]]['e']\n",
    "        \n",
    "        #check error\n",
    "        if (e > e_en * e_0 and len(ls) > MIN_EXPANSION_SIZE and num_deleted_neurons < 3):\n",
    "        \n",
    "            #subgraph\n",
    "            H = G.subgraph(ls)\n",
    "            \n",
    "            #recursively run algorithm to create new network for subgraph of this neurons nodes\n",
    "            n, e = ghsom(H, lam, eta, sigma, e, e_sg, e_en, init, layer + 1)\n",
    "            \n",
    "            #repack\n",
    "            network.node[network.nodes()[i]]['e'] = e\n",
    "            network.node[network.nodes()[i]]['n'] = n\n",
    "            \n",
    "        #increase overall network error\n",
    "        MQE += e\n",
    "    \n",
    "    #mean MQE\n",
    "    MQE /= nx.number_of_nodes(network)\n",
    "    \n",
    "    #return network\n",
    "    return network, MQE\n",
    "\n",
    "\n",
    "def unassign_all_nodes(G, labels):\n",
    "\n",
    "    #number of layers of communities\n",
    "    num_layers = len(labels)\n",
    "\n",
    "    for l in range(num_layers):\n",
    "\n",
    "        nx.set_node_attributes(G, 'community'+str(l), 'unassigned')\n",
    "\n",
    "##function to recursively label nodes in graph\n",
    "def label_graph(G, network, layer, neuron_count):\n",
    "    \n",
    "    for i in network.nodes():\n",
    "        \n",
    "        #unpack\n",
    "        l = network.node[i]['ls']\n",
    "        \n",
    "        for node in l:\n",
    "            G.node[node]['community'+str(layer)] = neuron_count[layer]\n",
    "            \n",
    "        n = network.node[i]['n']\n",
    "            \n",
    "        if len(n) > 0: \n",
    "            \n",
    "            H = G.subgraph(l)\n",
    "            \n",
    "            label_graph(H, n, layer + 1, neuron_count)\n",
    "            \n",
    "        neuron_count[layer] += 1\n",
    "\n",
    "\n",
    "##function to calculate community detection error given a generated benchmark graph\n",
    "def mutual_information(G, labels):\n",
    "    \n",
    "    #number of layers of communities\n",
    "    num_layers = len(labels)\n",
    "    \n",
    "    #initialise scores\n",
    "    scores = np.zeros(num_layers)\n",
    "    \n",
    "    #iterate over all levels of labels\n",
    "    for i in range(num_layers):\n",
    "    \n",
    "        #assigned first layer community\n",
    "        actual_community = nx.get_node_attributes(G, labels[num_layers - i - 1])\n",
    "\n",
    "        #predicted first layer community\n",
    "        predicted_community = nx.get_node_attributes(G, 'community'+str(i + 1))\n",
    "\n",
    "        #only retrieve labels for assigned nodes\n",
    "        labels_true = [v for k,v in actual_community.items() if k in predicted_community]\n",
    "        labels_pred = [v for k,v in predicted_community.items() if k in actual_community]\n",
    "        \n",
    "        if len(labels_pred) == 0:\n",
    "            continue\n",
    "            \n",
    "        #mutual information to score classifcation -- scale by number of assigned nodes out of all nodes\n",
    "        score = met.normalized_mutual_info_score(labels_true, labels_pred) * len(labels_pred) / len(actual_community)\n",
    "        scores[i] = score\n",
    "    \n",
    "    #return\n",
    "    return scores \n",
    "\n",
    "\n",
    "## get embedding\n",
    "def get_embedding(G):\n",
    "    \n",
    "    #get number of niodes in the graph\n",
    "    num_nodes = nx.number_of_nodes(G)\n",
    "    \n",
    "    #dimension of embedding\n",
    "    d = 0\n",
    "    \n",
    "    while 'embedding'+str(d) in G.node[G.nodes()[0]]:\n",
    "        d += 1\n",
    "    \n",
    "    #initialise embedding\n",
    "    X = np.zeros((num_nodes, d))\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        for j in range(d):\n",
    "            X[i,j] = G.node[G.nodes()[i]]['embedding'+str(j)]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def modularity(G, H):\n",
    "    \n",
    "    #number of links in communitiy H\n",
    "    l_s = nx.number_of_edges(H)\n",
    "    \n",
    "    #total degree of communitiy H\n",
    "    d_s = np.sum(list(H.degree().values()))\n",
    "    \n",
    "    #number of links in G\n",
    "    L = nx.number_of_edges(G)\n",
    "        \n",
    "    #modularitiy\n",
    "    Q = l_s / L - (d_s / (2 * L)) ** 2\n",
    "    \n",
    "    return Q\n",
    "\n",
    "##function to visualise graph\n",
    "def visualise_graph(G, colours, layer):\n",
    "        \n",
    "    ## create new figure for graph plot\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # graph layout\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    #attributes in this graph\n",
    "    attributes = np.unique([v for k,v in nx.get_node_attributes(G, 'community'+str(layer)).items()])\n",
    "\n",
    "    # draw nodes -- colouring by cluster\n",
    "    for i in range(min(len(colours), len(attributes))):\n",
    "       \n",
    "        node_list = [n for n in G.nodes() if G.node[n]['community'+str(layer)] == attributes[i]]\n",
    "        colour = [colours[i] for n in range(len(node_list))]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=node_list, node_color=colour)\n",
    "        \n",
    "    #draw edges\n",
    "    nx.draw_networkx_edges(G, pos)\n",
    "\n",
    "    # draw labels\n",
    "    nx.draw_networkx_labels(G, pos, )\n",
    "    \n",
    "    #title of plot\n",
    "    plt.title('Nodes coloured by cluster, layer: '+str(layer))\n",
    "\n",
    "    #show plot\n",
    "    plt.show()\n",
    "\n",
    "## visualise graph based on network clusters\n",
    "def visualise_network(network, colours, layer):\n",
    "    \n",
    "    #num neurons in lattice\n",
    "    num_neurons = len(network)\n",
    "\n",
    "    ##create new figure for lattice plot\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # graph layout\n",
    "    pos = nx.spring_layout(network)\n",
    "\n",
    "    # draw nodes -- colouring by cluster\n",
    "    for i in range(len(colours)):\n",
    "        nx.draw_networkx_nodes(network, pos, nodelist = [network.nodes()[i]], node_color = colours[i])\n",
    "\n",
    "    #draw edges\n",
    "    nx.draw_networkx_edges(network, pos)\n",
    "\n",
    "    # draw labels\n",
    "    nx.draw_networkx_labels(network, pos)\n",
    "    \n",
    "    #label axes\n",
    "    plt.title('Neurons in lattice, layer: '+str(layer))\n",
    "    \n",
    "    #show lattice plot\n",
    "    plt.show()\n",
    "\n",
    "###evaluate fitness\n",
    "def fitness(eta, sigma, e_sg, e_en, gml_filename, labels, init, lam):\n",
    "    \n",
    "    G = nx.read_gml(gml_filename)\n",
    "    labels = labels.split(',')\n",
    "\n",
    "    #start layer\n",
    "    layer = 0\n",
    "    \n",
    "    #run ghsom algorithm\n",
    "    network, MQE = ghsom(G, lam, eta, sigma, np.inf, e_sg, e_en, init, layer)\n",
    "\n",
    "    #label graph\n",
    "    neurons = np.zeros(MAX_DEPTH + 1, dtype=np.int)\n",
    "    unassign_all_nodes(G, labels)\n",
    "    label_graph(G, network, layer, neurons)\n",
    "\n",
    "    ##calculate error\n",
    "    mi_score = mutual_information(G, labels)\n",
    "        \n",
    "    n, d = network.nodes(data=True)[0]\n",
    "    \n",
    "    num_communities_detected = len(d['n'].nodes())\n",
    "    \n",
    "    return mi_score, num_communities_detected\n",
    "\n",
    "def main(params, gml_filename, labels, init=1, lam=10000):\n",
    "\n",
    "    return fitness(params['eta'], params['sigma'],\n",
    "                   params['e_sg'], params['e_en'], gml_filename, labels, init, lam)\n",
    "\n",
    "def main_no_labels(params, gml_filename, init=1, lam=10000):\n",
    "    \n",
    "    G = nx.read_gml(gml_filename)\n",
    "    \n",
    "    #start layer\n",
    "    layer = 0\n",
    "    \n",
    "    X = get_embedding(G)\n",
    "    \n",
    "    m = np.mean(X, axis=0)\n",
    "    MQE_0 = np.mean([np.linalg.norm(x - m) for x in X])\n",
    "\n",
    "    #run ghsom algorithm\n",
    "    network, MQE = ghsom(G, lam, params['eta'],\n",
    "                         params['sigma'], MQE_0, params['e_sg'], params['e_en'], init, layer)\n",
    "    \n",
    "    return G, network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'eta': 0.0001,\n",
    "         'sigma': 1,\n",
    "          'e_sg': 0.8,\n",
    "         'e_en': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1, training epoch: 9999/10000, size of map: 3, network size: 176, MQE: 5.44510741618, target: 5.38666973318                     "
     ]
    }
   ],
   "source": [
    "# %%time \n",
    "%prun G, networks = main_no_labels(params=params, gml_filename=\"embedded_yeast_uetz.gml\", lam=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_1(x, v):\n",
    "    \n",
    "    for i in range(10000):\n",
    "        \n",
    "        v += np.array([0.0001]) * x - v\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = np.random.uniform(size=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04796312,  0.38646151,  0.49529617,  0.56798658,  0.43206653,\n",
       "         0.17356835,  0.29435978,  0.8288629 ,  0.62234394,  0.74051404],\n",
       "       [ 0.34002349,  0.18842629,  0.78818838,  0.44633586,  0.52650104,\n",
       "         0.35583285,  0.4596984 ,  0.83822048,  0.16855601,  0.00920292],\n",
       "       [ 0.77386318,  0.17435395,  0.45648262,  0.79513426,  0.70997831,\n",
       "         0.04569377,  0.10352529,  0.71342037,  0.56106793,  0.27260139],\n",
       "       [ 0.09635042,  0.52094695,  0.01411746,  0.39399231,  0.94360131,\n",
       "         0.64572215,  0.43785749,  0.3513552 ,  0.27922629,  0.46924484],\n",
       "       [ 0.91724523,  0.18708568,  0.5267329 ,  0.05651305,  0.88911102,\n",
       "         0.69810843,  0.51243033,  0.21123448,  0.41441248,  0.49245004],\n",
       "       [ 0.58438655,  0.96363878,  0.70915299,  0.00354427,  0.35951942,\n",
       "         0.69288692,  0.34674734,  0.96384332,  0.47401215,  0.94988029],\n",
       "       [ 0.5734977 ,  0.75685148,  0.10582581,  0.73503512,  0.33230438,\n",
       "         0.28178631,  0.67826522,  0.31588904,  0.31966276,  0.90301622],\n",
       "       [ 0.06379677,  0.64589609,  0.81305973,  0.02504455,  0.7058676 ,\n",
       "         0.84531656,  0.78121643,  0.51226416,  0.9852907 ,  0.00438044],\n",
       "       [ 0.72299592,  0.28476317,  0.08142789,  0.51016912,  0.34051087,\n",
       "         0.14134941,  0.6228984 ,  0.37499379,  0.39992516,  0.8047215 ],\n",
       "       [ 0.42332756,  0.04023479,  0.60135568,  0.91018003,  0.16884277,\n",
       "         0.18210266,  0.76319376,  0.10584647,  0.24075245,  0.05835475]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 69.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit train_1(x, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_2(x, v):\n",
    "    \n",
    "    return np.array([v])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
