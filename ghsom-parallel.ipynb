{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import sklearn.metrics as met\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "from Queue import Queue\n",
    "from threading import Thread\n",
    "from threading import current_thread\n",
    "\n",
    "NUM_THREADS = 5\n",
    "MIN_EXPANSION_SIZE = 10\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "#function to generate real valued som for graph input\n",
    "def initialise_network(X):\n",
    "    \n",
    "    #network will be a one dimensional list\n",
    "    network = nx.Graph()\n",
    "    \n",
    "    #initialise a network with just one neuron\n",
    "    network.add_node(0)\n",
    "    \n",
    "    #assign a random vector in X to be the weight\n",
    "    network.node[i][\"v\"] = X[np.random.randint(len(X))]\n",
    "    \n",
    "    #return network\n",
    "    return network\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "# function to train SOM on given graph\n",
    "def train_network(X, network, num_epochs, eta_0, sigma_0):\n",
    "    \n",
    "    #initial learning rate\n",
    "    eta = eta_0\n",
    "    \n",
    "    #initial neighbourhood size\n",
    "    sigma = sigma_0\n",
    "    \n",
    "    #list if all patterns to visit\n",
    "    training_patterns = range(len(X))\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        #shuffle nodes\n",
    "        np.random.shuffle(training_patterns)\n",
    "        \n",
    "        # iterate through N nodes of graph\n",
    "        for i in trainingPatterns:\n",
    "            \n",
    "            #data point to consider\n",
    "            x = X[i]\n",
    "            \n",
    "            #determine winning neuron\n",
    "            closest_neuron = winning_neuron(x, network)\n",
    "            \n",
    "            # update weights\n",
    "            update_weights(x, network, closest_neuron, eta, sigma)\n",
    "            \n",
    "        # drop neighbourhood\n",
    "        sigma = sigma_0 * np.exp(-2 * sigma_0 * e / num_epochs);\n",
    "        \n",
    "##########################################################################################################################\n",
    "\n",
    "# winning neuron\n",
    "def winning_neuron(x, network):\n",
    "    \n",
    "    distances = {n: np.linalg.norm(x - d[\"v\"]) for n, d in network.nodes(data=True)}\n",
    "    \n",
    "    return min(distances, key=distances.get)\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "# function to update weights\n",
    "def update_weights(x, network, winning_neuron, eta, sigma):\n",
    "        \n",
    "    shortest_path_length = np.array([v for k, v in nx.shortest_path_length(network, source=winning_neuron).items()])\n",
    "    \n",
    "    weights = np.array([v for n, v in nx.get_node_attributes(network, \"v\").items()])\n",
    "    weights += eta * np.exp(- shortest_path_length ** 2 / (2 * sigma ** 2)) * (x - weights)\n",
    "    \n",
    "    nx.set_node_attributes(network, \"v\", {n: v for n, v in zip(network.nodes(), weights)})\n",
    "\n",
    "###########################################################################################################################\n",
    "    \n",
    "# assign nodes into clusters\n",
    "def assign_nodes(names, X, network):\n",
    "    \n",
    "    distances = np.array([[np.linalg.norm(d[\"v\"] - x) for n, d in network.nodes(data=True)] for x in X])\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "    assignments = np.argmin(distances, axis=1)\n",
    "    \n",
    "    errors = np.array([np.mean(distances[assignments == n]) for n in network.nodes()])\n",
    "    \n",
    "    ##TODO delete nodes with error == NAN\n",
    "    empty_neurons = np.where([np.isnan(errors)])[0]\n",
    "    \n",
    "    if empty_neurons.size > 0:\n",
    "    \n",
    "        ################################################DELETION####################################################\n",
    "\n",
    "        #neighbours of deleted neurons\n",
    "        neighbour_lists = np.array([network.neighbors(n) for n in empty_neurons])\n",
    "\n",
    "        #remove the nodes\n",
    "        network.remove_node_from(empty_neurons)\n",
    "        \n",
    "        ##connect separated components\n",
    "        for neighbour_list in neighbour_lists:\n",
    "            connect_components(network, neighbour_list)\n",
    "\n",
    "        #########################################################################################################\n",
    "    \n",
    "    MQE = np.mean(errors)\n",
    "    \n",
    "    errors = {n: e for n, e in zip(network.nodes(), errors)}\n",
    "    assignments = {n: [names[i] for i in np.where(assignments==n)] for n in network.nodes()}\n",
    "    \n",
    "    nx.set_node_attributes(network, \"e\", errors)\n",
    "    nx.set_node_attributes(network, \"ls\", assignments)\n",
    "    \n",
    "    return MQE, len(empty_neurons)\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "def connect_components(network, neighbour_list):\n",
    "    \n",
    "    sub_network = network.subgraph(neighbour_list)\n",
    "    \n",
    "    connected_components = [c for c in nx.connected_components(sub_network)]\n",
    "    number_of_connected_components = len(connected_components)\n",
    "    \n",
    "    for i in range(number_of_connected_components):\n",
    "        \n",
    "        for j in range(i + 1, number_of_connected_components):\n",
    "            \n",
    "            distances = np.array([[np.linalg.norm(n1[\"v\"] - n2[\"v\"]) for n2 in connected_components[j]] \n",
    "                                  for n1 in connected_components[i]])\n",
    "            \n",
    "            min_n1, min_n2 = np.unravel_index(distances.argmin(), distances.shape)\n",
    "            \n",
    "            network.add_edge(connected_components[i][min_n1], \n",
    "                            connected_components[j][min_n2])\n",
    "\n",
    "##########################################################################################################################\n",
    "            \n",
    "##function to identify neuron with greatest error\n",
    "def identify_error_unit(network):\n",
    "    \n",
    "    errors = nx.get_node_attributes(network, \"e\")\n",
    "    \n",
    "    return max(errors, key=errors.get)\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "def expand_network(named_X, network, error_unit):\n",
    "    \n",
    "    #id of new node\n",
    "    id = max(network) + 1\n",
    "    \n",
    "    network.add_node(id)\n",
    "    \n",
    "    #v goes to random vector in range of error unit\n",
    "    ls = network.node[error_unit]['ls']\n",
    "    \n",
    "    r = np.random.randint(len(ls))\n",
    "    \n",
    "    v = named_X[ls[r]]\n",
    "    network.node[id]['v'] = v\n",
    "        \n",
    "    #identify neighbour pointing closet\n",
    "    error_unit_neighbours = network.neighbors(error_unit)\n",
    "    \n",
    "    if len(error_unit_neighbours) == 0:\n",
    "        \n",
    "        ##add edge\n",
    "        network.add_edge(error_unit, id)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        error_unit_vector = network.node[error_unit][\"v\"]\n",
    "        \n",
    "        ##find closest neighbour\n",
    "        distances = {n: np.linalg.norm(v - n[\"v\"]) for n in error_unit_neighbours}\n",
    "        closest_neighbour = min(distances, key=distances.get)\n",
    "        \n",
    "        #connect to error unit and closest neighbour\n",
    "        network.add_edge(closest_neighbour, id)\n",
    "        network.add_edge(error_unit, id)\n",
    "        \n",
    "##########################################################################################################################\n",
    "##########################################################################################################################\n",
    "\n",
    "##GHSOM algorithm\n",
    "def ghsom(ID, named_X, lam, eta, sigma, e_0, e_sg, e_en, q):\n",
    "    \n",
    "    #separate names and matrix of node embedding\n",
    "    names, X = zip(*named_X.items())\n",
    "    X = np.array(X)\n",
    "    \n",
    "    #create som for this neuron\n",
    "    network = initialise_network(X)\n",
    "    \n",
    "    #train for lamda epochs\n",
    "    train_network(X, network, lam, eta, sigma)\n",
    "\n",
    "    #classify nodes and compute error\n",
    "    MQE, num_deleted_neurons = assign_nodes(names, X, network)\n",
    "    \n",
    "    ##som growth phase\n",
    "    #repeat until error is low enough\n",
    "    while MQE > e_sg * e_0 and num_deleted_neurons < 3:\n",
    "    \n",
    "        #find neuron with greatest error\n",
    "        error_unit = identify_error_unit(network)\n",
    "        \n",
    "        #expand network\n",
    "        expand_network(named_X, network, error_unit)\n",
    "        \n",
    "        #train for lam epochs\n",
    "        train_network(X, network, lam, eta, sigma)\n",
    "\n",
    "        #calculate mean network error\n",
    "        MQE, deleted_neurons = assign_nodes(names, X, network)\n",
    "        num_deleted_neurons += deleted_neurons\n",
    "        \n",
    "        \n",
    "    ##neuron expansion phase\n",
    "    #iterate thorugh all neruons and find neurons with error great enough to expand\n",
    "    for i, d in network.node(data=True):\n",
    "        \n",
    "        #unpack\n",
    "        ls = d[\"ls\"]\n",
    "        e = d[\"e\"]\n",
    "        \n",
    "        #check error\n",
    "        if (e > e_en * e_0 and len(ls) > MIN_EXPANSION_SIZE and num_deleted_neurons < 3):\n",
    "            \n",
    "            id = \"{}-{}\".format(ID, str(i).zfill(2)) \n",
    "                \n",
    "            sub_X = {k: named_X[k] for k in ls}\n",
    "            \n",
    "            print \"submitted job: id={}, e={}\".format(id, e)\n",
    "            \n",
    "            #add these parameters to the queue\n",
    "            q.put((id, sub_X, lam, eta, sigma, e, e_sg, e_en))\n",
    "    \n",
    "    #return network\n",
    "    return network, MQE\n",
    "\n",
    "##########################################################################################################################\n",
    "##########################################################################################################################\n",
    "\n",
    "## get embedding TERRIBLE but staying\n",
    "def get_embedding(G):\n",
    "    \n",
    "    #get number of niodes in the graph\n",
    "    num_nodes = nx.number_of_nodes(G)\n",
    "    \n",
    "    #dimension of embedding\n",
    "    d = 0\n",
    "    while 'embedding'+str(d) in G.node[G.nodes()[0]]:\n",
    "        d += 1\n",
    "    \n",
    "    #initialise embedding\n",
    "    X = np.zeros((num_nodes, d))\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        for j in range(d):\n",
    "            X[i,j] = G.node[G.nodes()[i]]['embedding'+str(j)]\n",
    "    \n",
    "    return X\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "def worker(q, networks):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        #unpack first element of queue\n",
    "        #conatains all the para,eters for GHSOM\n",
    "        ID, X, lam, eta, sigma, e_0, e_sg, e_en = q.get()\n",
    "        \n",
    "        #run GHSOM and return a network and MQE\n",
    "        n, e = ghsom(ID, X, lam, eta, sigma, e_0, e_sg, e_en, q)\n",
    "        \n",
    "        #append result to networks list\n",
    "        networks.append((ID, n, e))\n",
    "        \n",
    "        #mark task as done\n",
    "        q.task_done()\n",
    "\n",
    "def main(params, gml_filename, lam=10000):\n",
    "    \n",
    "    #network\n",
    "    G = nx.read_gml(gml_filename)\n",
    "    \n",
    "    #embedding matrix\n",
    "    X = get_embedding(G)\n",
    "    \n",
    "    #zip with names\n",
    "    named_X = {k: v for k, v in zip(G.nodes(), X)}\n",
    "    \n",
    "    ##list of returned networks\n",
    "    networks = []\n",
    "    \n",
    "    #initilise worker queue\n",
    "    q = Queue()\n",
    "    \n",
    "    #initialise threads\n",
    "    for i in range(NUM_THREADS):\n",
    "        \n",
    "        t = Thread(target=worker, args=(q, networks))\n",
    "        t.setDaemon(True)\n",
    "        t.start()\n",
    "        \n",
    "    ##initial MQE is variance of dataset\n",
    "    m = np.mean(X, axis=0)\n",
    "    MQE_0 = np.mean([np.linalg.norm(x - m) for x in X])\n",
    "        \n",
    "    #add initial layer of ghsom to queue\n",
    "    q.put((\"01\", X, lam, params[\"eta\"], params[\"sigma\"], MQE_0, params[\"e_sg\"], params[\"e_en\"]))\n",
    "    \n",
    "    #finally wait until queue is empty and all tasks are done\n",
    "    q.join()\n",
    "    \n",
    "    print \"DONE\"\n",
    "    \n",
    "    return G, networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'eta': 0.0001,\n",
    "         'sigma': 1,\n",
    "          'e_sg': 0.8,\n",
    "         'e_en': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "%prun G, networks = main(params=params, gml_filename=\"embedded_yeast_uetz.gml\", lam=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
